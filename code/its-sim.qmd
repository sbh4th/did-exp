---
title: "ITS treatment considerations"
format: 
  html:
    code-fold: true
    cache: true
---
## Data setup

First let's simulate some data. Although this is an ITS analysis that usually would have two time series, I've simulated a bunch of units for each treatment group to generate some sampling error. We'll simulate 1000 observations from 30 different groups (e.g., states) over a period of 20 years, but in this case every single group is treated (i.e., there are no 'never treated' units.). There are two treatment cohorts that are treated at different times, one in 2010 and the other in 2015. The 2015 cohort is twice as large as the 2010 cohort, just to include some variation. Each group has both a different underlying secular trend and a different treatment effect.

```{r, message = FALSE}
# load packages
library(here)
library(tidyverse)
library(estimatr)
library(modelsummary)
library(marginaleffects)
library(tinytable)

# set seed for reproducibility
set.seed(95478)

# unit fixed effects (unobserved heterogeneity)
unit <- tibble(
  unit = 1:1000,
  # generate clusters
  state = sample(1:30, 1000, replace = TRUE),
  unit_fe = rnorm(1000, state/10, 1),
  # generate instantaneous treatment effect
  #mu = rnorm(nobs, true_mu, 0.2)
  mu = 2
)

# year fixed effects (first part)
year <- tibble(
  year = 2001:2020,
  year_fe = rnorm(length(year), 0, 0.5)
)

# Put the clusters into treatment groups
treat_taus <- tibble(
  # sample the clusters randomly
  state = sample(1:30, 30, replace = FALSE),
  # place the randomly sampled states into 1\{t \ge g \}G_g
  cohort_year = sort(rep(c(2010, 2015), times=c(10,20)))
)

# make main dataset
# full interaction of unit X year 
dtest <- expand_grid(unit = 1:1000, year = 2001:2020) %>% 
  left_join(., unit) %>% 
  left_join(., year) %>% 
  left_join(., treat_taus) %>% 
  # make error term and get treatment indicators and treatment effects
  # Also get cohort specific trends (modify time FE)
  mutate(error = rnorm(1000*20, 0, 2),
         treat = ifelse((year >= 
           cohort_year), 1, 0),
         # treatment effect = 4 if 2015, 1 if 2010, annually
         mu = ifelse(cohort_year==2015, 4, 1),
         tau = ifelse(treat == 1, mu, 0),
         # year trends differ by cohort
         year_fe = year_fe + (2020 - cohort_year) * 
           (year - cohort_year) / 5 + 20
  ) %>% 
  # calculate cumulative treatment effects
  group_by(unit) %>% 
  mutate(tau_cum = cumsum(tau)) %>% 
  ungroup() %>% 
  # calculate the dependent variable
  mutate(y = (2020 - cohort_year) + 
           unit_fe + year_fe + tau_cum + error) 
  # Relabel 2018 cohort as never-treated
  # mutate(cohort_year = ifelse(cohort_year == 2018, Inf, cohort_year))

# bring in simulated DiD dataset
ds <- dtest %>%
  # read_rds(here("data", 
 #"did-sim-data.rds")) %>%
  mutate(year0 = year - 2001,
    yearc = year - cohort_year,
    since = treat * yearc,
    cohort_2015 = ifelse(cohort_year == 2015, 1, 0))
```

Here is a plot of the simulated data, with the averages plotted for each cohort and the time of intervention marked on the graph. The specific treatment effects are `r mean(ds$tau[ds$cohort_year==2010 & ds$treat==1])` for the 2010 cohort and `r mean(ds$tau[ds$cohort_year==2015 & ds$treat==1])` for the 2015 cohort.

```{r, message=FALSE, echo=F}
ds %>% 
  ggplot(aes(x = year0, y = y, group = unit)) + 
  geom_line(alpha = 1/8, color = "grey") + 
  geom_line(data = ds %>% 
    group_by(cohort_year, year0) %>% 
    summarize(y = mean(y)),
   aes(x = year0, y = y, group = factor(cohort_year),
    color = factor(cohort_year)), linewidth = 2) + 
  geom_line(data = ds %>%
    group_by(year0) %>%
      summarize(y = mean(y)),
    aes(x = year0, y = y, color = "#4daf4a", group="All"), 
    linewidth = 1, linetype = 2) +
  labs(x = "", y = "Y",  color = "Treatment group   ") +
  scale_x_continuous(breaks = c(5, 10, 15, 20)) +
  geom_vline(xintercept = 9, color = '#E41A1C', linewidth = 2) + 
  geom_vline(xintercept = 14, color = '#377EB8', linewidth = 2) + 
  scale_color_brewer(palette = 'Set1', 
    labels = c("2010", "2015", "Average")) + 
  theme_classic()
```

## Group-specific analysis

What is the causal effect of the intervention in the 2010 cohort? 

Let's do the ITS for the early treated cohort with a simple model like:^[Note this model is set up for an ITS where the product term uses the 'time-since intervention', i.e. `since` term. This is calculated as year0 - year0 at time of intervention].

$$Y_{it} = \beta_{0} + \beta_{1}*year0 + \beta_{2}*treat + \beta_{3}*since + \epsilon_{it}$$

First, let's estimate the models separately for each cohort. We'll use the `lm_robust` function from `estimatr` to easily get cluster robust standard errors. 

```{r}
# models
m_c2010 <- lm_robust(y ~ year0 + treat + since, 
  data = subset(ds, cohort_year == 2010), 
  clusters = state)

m_c2015 <- lm_robust(y ~ year0 + treat + since, 
  data = subset(ds, cohort_year == 2015), 
  clusters = state)

modelsummary(list("2010 cohort" = m_c2010,
                  "2015 cohort" = m_c2015),
  fmt = 2, statistic = "conf.int", 
  shape = term ~ model + statistic,
  gof_omit = 'DF|Deviance|R2|AIC|BIC|RMSE')
```

The slope estimates are basically what we specified (with sampling error) in the simulation (`r mean(ds$tau[ds$cohort_year==2010 & ds$treat==1])` for the 2010 cohort and `r mean(ds$tau[ds$cohort_year==2015 & ds$treat==1])` for the 2015 cohort). Now let's estimate the effect of the intervention at the end of follow-up, setting `year0=19` and estimating the difference by treatment status, using the `marginaleffects` package. We'll estimate the average predictions under each treatment regime and then calculate the difference.

```{r, warning = FALSE}
# predictions for 2010 cohort
p_2010 <- predictions(m_c2010, 
  newdata = datagrid(year0 = c(19,19),
    treat = c(0,1), since = c(0,10))) 

d_2010 <- hypotheses(p_2010, "b4 - b1 = 0")

p_2010 <- p_2010 %>%
  filter(rowid==c(1,4)) %>%
  bind_rows(d_2010) %>%
  mutate(term = c("Untreated", "Treated",
    "Difference"))

# predictions for 2015 cohort
p_2015 <- predictions(m_c2015, 
  newdata = datagrid(year0 = c(19,19),
    treat = c(0,1), since = c(0,5)))
d_2015 <- hypotheses(p_2015, "b4 - b1 = 0")

p_2015 <- p_2015 %>% 
    filter(rowid==c(1,4)) %>%
  bind_rows(d_2015) %>%
  mutate(term = c("Untreated", "Treated",
    "Difference"))

modelsummary(list("2010 cohort" = p_2010,
  "2015 cohort" = p_2015), fmt=2,
  statistic = "conf.int", shape = term ~ statistic)
```

So, we get predictions of `r round(p_2010$estimate[2], 1)` under treatment and `r round(p_2010$estimate[1], 1)` in the counterfactual absence of treatment for the 2010 cohort. The difference is `r round(p_2010$estimate[3], 1)`. For the 2015 later-treated cohort, at the end of follow-up we get predictions of `r round(p_2015$estimate[2], 1)` under treatment and `r round(p_2015$estimate[1], 1)` in the counterfactual absence of treatment. The difference is `r round(p_2015$estimate[3], 1)`. If we wanted to ignore any heterogeneity and just calculate an average causal effect at the end of follow-up, we can just take a weighted average of the cohort-specific effects.

```{r}
te <- p_2010$estimate[3]*(1/3) + p_2015$estimate[3]*(2/3)
```

The average causal effect in the population is `r round(te, 2)`. 

In each of these cases it is clear that we are comparing each treated unit to itself to generate these counterfactual estimates. We could also use `marginaleffects` to get the average annual slopes (though redundant since these are coefficients in the models). Although we fit these models separately, we could hand calculate an overall averaged effect (ignoring cohort heterogeneity). The 2015 cohort is twice as large as the 2010 cohort, so a weighted average of the average treatment slopes above is (1/3) * `r round(m_c2010$coefficients[4], 2)` + (2/3) * `r round(m_c2015$coefficients[4], 2)`, which is about `r round((1/3) * m_c2010$coefficients[4] + (2/3) * m_c2015$coefficients[4], 2)`. 

## Pooled analysis with calendar time

Now let's analyze the entire cohort pooled together. Including a fixed effect for cohort in these models will only allow for an overall level shift, and if we include a product term between cohort and time, and cohort and the level change (`treat`) and cohort and the slope change `since`), we can recover the causal effects in both cohorts, or averaged over the entire period. Here are all of the model estimates:

```{r}

m_all <- lm_robust(formula = 
  y ~ year0 + treat + since, 
  data = ds, clusters = state)

m_all_s <- lm_robust(formula = 
  y ~ cohort_2015 + year0 + treat + since, 
  data = ds, clusters = state)

m_all_int <- lm_robust(formula = 
  y ~ cohort_2015 * (year0 + treat + since), 
  data = ds, clusters = state)

modelsummary(list("2010 cohort" = m_c2010,
  "2015 cohort" = m_c2015, "No cohort FE" = m_all,
  "Cohort FE" = m_all_s, "Interactive" = m_all_int),
  fmt = 2, gof_omit = 'DF|Deviance|R2|AIC|BIC|RMSE')
```

If we use the interacted model we can recover the cohort-specific treatment effects at the end of follow-up above (again, setting `year0` to 19), as well as getting an overall average (again, choosing to ignore heterogeneity). 

```{r, warning=FALSE}
# predictions at end of follow-up by treatment
# set up dataframe for predictions
eof <- tibble(
  cohort_2015 = c(0,0,1,1),
  year0 = c(rep(19,4)),
  treat = c(0,1,0,1),
  since = c(0,10,0,5)
)

m_all_int_ap <- predictions(m_all_int, 
  newdata = eof)

# treatment effects
m_all_int_te <- hypotheses(m_all_int_ap, 
  c("b2 - b1 = 0", "b4 - b3 = 0")) %>%
  mutate(term = c("2010 cohort", "2015 cohort"))

modelsummary(list("Marginal effects from interaction model" 
  = m_all_int_te), fmt=2,
  statistic = "conf.int", shape = term ~ statistic,
  notes = "Note: calculated at end of follow-up.")
```

Same estimates that we calculated in the separate models above. The average slopes for each cohort can be estimated from the coefficients in the interacted model above. Since `cohort_2015`=0 for the 2010 cohort, the term `treat` gives the level shift at the time of the intervention for the 2010 cohort and the `since` term gives the post-intervention slope for the 2010 cohort. The product terms between `cohort_2015` and each of these terms give the *difference* between cohorts in the level shift and post-intervention slopes, respectively. You can get the cohort-specific slopes from this single model using the  `avg_slopes` function from `marginaleffects`:

```{r, warning = FALSE}

m_all_int_slopes <- avg_slopes(m_all_int, 
  variable=c("treat","since"), 
  by="cohort_2015") %>%
  mutate(
    term = c("Post-slope (since)","Post-slope (since)",
             "Level shift (treat)","Level shift (treat)"),
    cohort_2015 = factor(cohort_2015,
    labels = c("2010 cohort","2015 cohort")))

modelsummary(list("Average slopes: Interaction model" 
                  = m_all_int_slopes),
  fmt = 2, statistic = "conf.int", 
  shape = term ~ cohort_2015 + statistic,
  gof_omit = 'DF|Deviance|R2|AIC|BIC|RMSE')
```

Again, same as above. 

As noted, since the 2015 cohort is twice as large as the 2010 cohort, if we wanted an overall effect the weighted average effect can be derived by setting the value of `cohort_2015` to (2/3). Of course this average effect is a combination of the counterfactual effects at 10 years post-intervention for the 2010 cohort and 5 years post-intervention for the 2015 cohort.

```{r}
# predictions at end of follow-up
# averaging over the entire sample
m_all_int_aap <- hypotheses(m_all_int_ap, 
  c("(1/3)*b1 + (2/3)*b3 = 0", 
  "(1/3)*b2 + (2/3)*b4 = 0")) %>%
  mutate(term = c("Untreated", "Treated"))

m_all_int_ate <- hypotheses(m_all_int_ap, 
  "(1/3)*(b2 - b1) + (2/3)*(b4 - b3) = 0") %>%
  mutate(term = "Difference")

m_all_int_ates <- m_all_int_aap %>% 
  bind_rows(m_all_int_ate)

modelsummary(list("Population average effect: interaction model" 
    = m_all_int_ates),
  fmt = 2, statistic = "conf.int", 
  shape = term ~ model + statistic,
  gof_omit = 'DF|Deviance|R2|AIC|BIC|RMSE',
  notes = "Note: cohort fixed effect set to 2/3.")

```

Pretty much the same average predictions and treatment effects as we calculated above. 

Now what about the treatment effects for the other models? If we ignore cohort all together the treatment effect at the intervention point is `r round(m_all$coefficients[3],2)`, and the post-intervention slope is `r round(m_all$coefficients[4],2)`, which are considerably off from our population weighted values. Since the model that includes a cohort fixed effect only allows for an intercept difference, the estimates from this model aren't much better. In addition, these models guarantee a positivity violation, since there is zero probability that the 2015 cohort can be treated before 2015 or that the 2010 cohort can be untreated after 2010. 

## Relative time

Now let's work with relative time. First, let's plot the data.

```{r, message = FALSE}
ds %>%
  ggplot(aes(x = yearc, y = y, group = unit)) + 
  geom_line(alpha = 1/8, color = "grey") + 
  geom_line(data = ds %>% 
    group_by(cohort_year, yearc) %>% 
    summarize(y = mean(y)),
   aes(x = yearc, y = y, group = factor(cohort_year),
    color = factor(cohort_year)), linewidth = 2) +
  geom_line(data = ds %>%
    group_by(yearc) %>%
      summarize(y = mean(y)),
    aes(x = yearc, y = y, color = "#4daf4a", group="All"), 
    linewidth = 1, linetype = 2) +
  labs(x = "", y = "Y",  color = "Treatment group   ") +
  scale_x_continuous(breaks = c(-5, 0, 5, 10)) +
  geom_vline(xintercept = 0, color = '#984ea3', linewidth = 2) + 
  scale_color_brewer(palette = 'Set1', labels = c("2010", "2015", "Average")) + theme_classic()
```

Yeah, this looks a little strange, and you can see that, of course, the average slope in the early period is only coming from the 2015 cohort, and the slope in the later post period is only coming from the 2010 cohort. 

Now let's estimate the same models as we did above:

```{r}
m_c2010_c <- lm_robust(
  y ~ yearc + treat + since, 
  data = subset(ds, cohort_2015==0), 
  clusters = state)

m_c2015_c <- lm_robust(
  y ~ yearc + treat + since, 
  data = subset(ds, cohort_2015==1), 
  clusters = state)

m_all_c <- lm_robust(
  y ~ yearc + treat + since, 
  data = ds, clusters = state)

m_all_s_c <- lm_robust(
  y ~ cohort_2015 + yearc + treat + since, 
  data = ds, clusters = state)

m_all_int_c <- lm_robust(
  y ~ cohort_2015 * (yearc + treat + since), 
  data = ds, clusters = state)

modelsummary(list("2010 cohort" = m_c2010_c,
  "2015 cohort" = m_c2015_c, "No cohort FE" = m_all_c,
  "Cohort FE" = m_all_s_c, "Interactive" = m_all_int_c),
  fmt = 2, gof_omit = 'DF|Deviance|R2|AIC|BIC|RMSE')
```

Not surprisingly, since the cohort-specific models are basing the counterfactual off extrapolating from the *same* unit, the effects at the time of intervention and the post-intervention slopes are correct, but the intercept terms are now off because we have recentered year.^[In fact, you'll note that the estimated intercepts for these models are exactly the predicted outcomes for the non-centered models at the time of the intervention.] 

The pooled models with and without the cohort fixed effects give both the incorrect treatment effect and the incorrect slope. And, of course, the fully interactive model also gives identical estimates to what we found above. 

Since we have recentered year we can't really get an estimate of average predicted treatment effects at the end of follow-up. But we can get estimates for each cohort at the end of their respective follow-ups:

```{r, warning = FALSE}
# predictions for 2010 cohort
p_2010_c <- predictions(m_c2010_c, 
  newdata = datagrid(yearc = c(10,10),
    treat = c(0,1), since = c(0,10))) 

d_2010_c <- hypotheses(p_2010_c, "b4 - b1 = 0")

p_2010_c <- p_2010_c %>%
  filter(rowid==c(1,4)) %>%
  bind_rows(d_2010_c) %>%
  mutate(term = c("Untreated", "Treated",
    "Difference"))

# predictions for 2015 cohort
p_2015_c <- predictions(m_c2015_c, 
  newdata = datagrid(yearc = c(5,5),
    treat = c(0,1), since = c(0,5)))
d_2015_c <- hypotheses(p_2015_c, "b4 - b1 = 0")

p_2015_c <- p_2015_c %>% 
    filter(rowid==c(1,4)) %>%
  bind_rows(d_2015_c) %>%
  mutate(term = c("Untreated", "Treated",
    "Difference"))

modelsummary(list("2010 cohort (at 10yrs)" = p_2010_c,
  "2015 cohort (at 5 yrs)" = p_2015_c), fmt=2,
  statistic = "conf.int", shape = term ~ statistic)
```

Exactly the same as we estimated using calendar time above. As we showed above, you can recover these same estimates using the fully interacted model, but the pooled models with and without the cohort fixed effect generate biased estimates of the true treatment effects. 

Once again, the pooled models wildly overestimate the level shift and do not give the correct post-intervention slopes. 

Okay, now let's limit our comparisons to only the years where we have sufficient overlap of years once we center things (i.e., where `yearc` >= -9 or <= 5). Here is a plot of the restricted data:

```{r, echo=F, message = FALSE}
# relative time with restriction
ds %>%
  filter(yearc >= -9 & yearc <= 5) %>%
  ggplot(aes(x = yearc, y = y, group = unit)) + 
  geom_line(alpha = 1/8, color = "grey") + 
  geom_line(data = ds %>% 
    filter(yearc >= -9 & yearc <= 5) %>%
    group_by(cohort_year, yearc) %>% 
    summarize(y = mean(y)),
   aes(x = yearc, y = y, group = factor(cohort_year),
    color = factor(cohort_year)), linewidth = 2) +
  geom_line(data = ds %>%
    filter(yearc >= -9 & yearc <= 5) %>%
    group_by(yearc) %>%
      summarize(y = mean(y)),
    aes(x = yearc, y = y, color = "#4daf4a", group="All"), 
    linewidth = 1, linetype = 2) +
  labs(x = "", y = "Y",  color = "Treatment group   ") +
  scale_x_continuous(breaks = c(-5, 0, 5, 10)) +
  geom_vline(xintercept = 0, color = '#984ea3', linewidth = 2) + 
  scale_color_brewer(palette = 'Set1', labels = c("2010", "2015", "Average")) + theme_classic()
```

Here are the models using relative time:

```{r}
m_c2010_cr <- lm_robust(
  y ~ yearc + treat + since, 
  data = subset(ds, cohort_2015==0 &
    yearc >= -9 & yearc <= 5), 
  clusters = state)

m_c2015_cr <- lm_robust(
  y ~ yearc + treat + since, 
  data = subset(ds, cohort_2015==1 &
    yearc >= -9 & yearc <= 5), 
  clusters = state)

m_all_cr <- lm_robust(
  y ~ yearc + treat + since, 
  data = subset(ds, yearc >= -9 & yearc <= 5), 
  clusters = state)

m_all_s_cr <- lm_robust(
  y ~ cohort_2015 + yearc + treat + since, 
  data = subset(ds, yearc >= -9 & yearc <= 5), 
  clusters = state)

m_all_int_cr <- lm_robust(
  y ~ cohort_2015 * (yearc + treat + since), 
  data = subset(ds, yearc >= -9 & yearc <= 5), 
  clusters = state)

modelsummary(list("2010 cohort" = m_c2010_cr,
  "2015 cohort" = m_c2015_cr, "No cohort FE" = m_all_cr,
  "Cohort FE" = m_all_s_cr, "Interactive" = m_all_int_cr),
  fmt = 2, gof_omit = 'DF|Deviance|R2|AIC|BIC|RMSE')
```

These estimates look good. The estimated level shifts and the post-intervention slopes are just about identical to what we got using calendar time. It's also interesting to see that the level and slope parameters from the pooled models with and without cohort fixed effects also give correct average level and slope parameters, which is in contrast to the calendar time models above. This is because recentering year and limiting the data to overlapping periods removes the extrapolation and positivity violations that were created using calendar time and ignoring heterogeneity by cohort.

As the model comparison shows, including just the cohort fixed effect here only allows for a 'level shift', so it has no impact on the estimated treatment effect at the time of intervention (`treat`) or the post-intervention slope (`since`). So these models will both produce the same average slopes (though not exactly the same average predictions because of the cohort fixed effect).

The average post-intervention slope for the model with cohort is `r round(m_all_s_cr$coefficients[4], 2)` (same for the model with cohort), and both of these estimates are nearly the same as the estimate we calculated as a weighted average for the full sample using calendar time, which was `r round((1/3) * m_c2010$coefficients[4] + (2/3) * m_c2015$coefficients[4], 2)`.

With the fully interacted model using restricted relative time, you get the same slope parameters for `treat`, `since` and their interactions with cohort, but the level parameters (e.g., intercept, cohort fixed effect) will be different. You can then back out predictions for a fixed time post-intervention (e.g., 10 years), but this will obviously lead to extrapolation for the later treated cohort. Nevertheless this shows that the relative time model after eliminating overlap *can* recover estimates of the true treatment effects. 

## Hang on...

The big caveat to the last point above is that this only occurs under specific kinds of scenarios. In particular, the restricted, centered time approach 'works' in recovering the true slopes in our prior example because the simulated trends are linear (though not identical across cohorts) in both of the pre- and post-treatment periods. It's intuitive in the sense that, if the trends are linear through the entire time, it shouldn't matter much whether you use 10 pre-periods or 5 pre-periods to estimate the pre-intervention slope (and vice versa for the post-intervention slope in the early treated cohort).

But this is a strong (and unnecessary) assumption, artificially reduces the sample size and restricts the population for making inferences, and does not allow for the same kinds of treatment effect estimation as when using calendar time. 

Plus, if there is non-linearity in the slopes then restricting the analysis to periods of overlap won't actually help to recover the 'true' effects. Let's slightly modify our example. We will keep the treatment effects identical to the last example but now introduce a small amount of non-linearity to the pre-intervention trends for the 2015 cohort.

Here is the code to tweak our previous simulation:
```{r, message = FALSE}
ds2 <- ds %>% 
  # add non-linearity to early part of 2015 cohort
  mutate(
    year_fe = ifelse(cohort_year==2015 & year<2006,
          year_fe + (2015 - cohort_year) * 
           (year - cohort_year) / 5 + 12,
          year_fe + (2020 - cohort_year) *
           (year - cohort_year) / 5 + 20), 
    # re-generate outcome
    y = (2020 - cohort_year) + 
           unit_fe + year_fe + tau_cum + error) 
```

Here is our new plot:
```{r, message = FALSE}
ds2 %>% 
  ggplot(aes(x = year0, y = y, group = unit)) + 
  geom_line(alpha = 1/8, color = "grey") + 
  geom_line(data = ds2 %>% 
    group_by(cohort_year, year0) %>% 
    summarize(y = mean(y)),
   aes(x = year0, y = y, group = factor(cohort_year),
    color = factor(cohort_year)), linewidth = 2) + 
  geom_line(data = ds2 %>%
    group_by(year0) %>%
      summarize(y = mean(y)),
    aes(x = year0, y = y, color = "#4daf4a", group="All"), 
    linewidth = 1, linetype = 2) +
  labs(x = "", y = "Y",  color = "Treatment group   ") +
  scale_x_continuous(breaks = c(5, 10, 15, 20)) +
  geom_vline(xintercept = 9, color = '#E41A1C', linewidth = 2) + 
  geom_vline(xintercept = 14, color = '#377EB8', linewidth = 2) + 
  scale_color_brewer(palette = 'Set1', 
    labels = c("2010", "2015", "Average")) + 
  theme_classic()
```

In this particular scenario, the non-linear part of the trend for the 2015 cohort is in the very early pre-period (everything else is the same), which means it won't be captured when we have to restrict to years of overlap when using relative time. Here is a plot using relative time:

```{r, message = FALSE}
ds2 %>%
  filter(yearc >= -9 & yearc <= 5) %>%
  ggplot(aes(x = yearc, y = y, group = unit)) + 
  geom_line(alpha = 1/8, color = "grey") + 
  geom_line(data = ds2 %>% 
    filter(yearc >= -9 & yearc <= 5) %>%
    group_by(cohort_year, yearc) %>% 
    summarize(y = mean(y)),
   aes(x = yearc, y = y, group = factor(cohort_year),
    color = factor(cohort_year)), linewidth = 2) +
  geom_line(data = ds2 %>%
    filter(yearc >= -9 & yearc <= 5) %>%
    group_by(yearc) %>%
      summarize(y = mean(y)),
    aes(x = yearc, y = y, color = "#4daf4a", group="All"), 
    linewidth = 1, linetype = 2) +
  labs(x = "", y = "Y",  color = "Treatment group   ") +
  scale_x_continuous(breaks = c(-5, 0, 5, 10)) +
  geom_vline(xintercept = 0, color = '#984ea3', linewidth = 2) + 
  scale_color_brewer(palette = 'Set1', labels = c("2010", "2015", "Average")) + theme_classic()
```
This looks a lot like the plot from our original simulation. We won't go through all of the prior models, but let's compare what we get from the interactive analysis using calendar time, as well as the simple model using restricted time, which we saw gave us the correct slope in the example above. Here are the models and parameters:

```{r}
# calendar time full model
m_all_int2 <- lm_robust(
  y ~ cohort_2015 * (year0 + treat + since),
  data = ds2, clusters = state)

# relative time full model
m_all_int_cr2 <- lm_robust(
  y ~ cohort_2015 * (yearc + treat + since), 
  data = subset(ds2, yearc >= -9 & yearc <= 5), 
  clusters = state)

ft <- modelsummary(list("Sim1: Interaction" = m_all_int, 
  "Sim2: Interaction" = m_all_int2,
  "Sim1: Interaction" = m_all_int_cr, 
  "Sim2: Interaction" = m_all_int_cr2),
  fmt = 2, coef_rename = c("year0" = "year", 
                  "yearc" = "year"),
  gof_omit = 'DF|Deviance|R2|AIC|BIC|RMSE',
  output = 'tinytable')

tt(ft@table_dataframe) %>%
  group_tt(j=list("Calendar time" = 2:3, 
    "Relative time (restricted)" = 4:5))
```

Since we only intervened with the pre-trend for the 2015 cohort, the `treat` and `since` values are identical in the interacted model using calendar time, since these are the treatment effects for the 2010 cohort. However, the `treat` effect for the 2015 cohort (i.e., $\beta*treat + \beta*cohort_{2015}*treat$) is now `r round(m_all_int2$coefficients[4] + m_all_int2$coefficients[7], 2)` instead of `r round(m_all_int$coefficients[4] + m_all_int$coefficients[7], 2)` and the post-intervention slope effect (`since`, i.e., $\beta*since + \beta*cohort_{2015}*since$) is now `r round(m_all_int2$coefficients[5] + m_all_int2$coefficients[8], 2)` instead of `r round(m_all_int$coefficients[5] + m_all_int$coefficients[8], 2)`. 

These differences in the true effects are due to the fact that introducing this bit of non-linearity in the early years flattens the pre-intervention slope for the 2015 cohort, leading to a larger jump at the intervention and greater slope in the post-intervention period. 

Above we saw that when using relative time and limiting the data to years of overlap even the simple models with or without a fixed effect for cohort could recover the weighted average treatment slope from the calendar time model. What do we get now?  

```{r}
# relative time basic model
m_all_cr2 <- lm_robust(
  y ~ yearc + treat + since,
  data=subset(ds2, yearc >= -9 & yearc <= 5), 
  clusters = state)

# relative time cohort FE model
m_all_s_cr2 <- lm_robust(
  y ~ cohort_2015 + yearc + treat + since,
  data=subset(ds2, yearc >= -9 & yearc <= 5), 
  clusters = state)

modelsummary(list("No cohort FE" = m_all_cr2,
  "Cohort FE" = m_all_s_cr2),
  fmt = 2, statistic = "conf.int", 
  shape = term ~ model + statistic,
  gof_omit = 'DF|Deviance|R2|AIC|BIC|RMSE')

```

From the interacted calendar time models above the true weighed average `treat` effect is `r round((1/3)*m_all_int2$coefficients[4] + (2/3)*m_all_int2$coefficients[7], 2)` and the weighted average `since` effect is `r round((1/3)*m_all_int2$coefficients[5] + (2/3)*m_all_int2$coefficients[8], 2)`. The estimates using the restricted relative time models with and without cohort fixed effects are identical: `r round(m_all_cr2$coefficients[3], 2)` for `treat` and `r round(m_all_cr2$coefficients[4], 2)` for `since`. These aren't accurate estimates. This is precisely because the pre-intervention slope of the restricted relative time model does not account for the diminished slope of the 2015 cohort in the early pre-period.

Even the interacted model using restricted relative time that allows for separate treatment effects by cohort does not recover the true treatment effects. Those (weighted average) estimates are `r round((1/3)*m_all_int_cr2$coefficients[4] + (2/3)*m_all_int_cr2$coefficients[7], 2)` for level effect and `r round((1/3)*m_all_int_cr2$coefficients[5] + (2/3)*m_all_int_cr2$coefficients[8], 2)` for the post-intervention slope effect. Again, the main issue is the fact that the average pre-intervention slope for the 2015 cohort is biased downward. Obviously this latter simulation does not introduce a lot of non-linearity in the pre-trends, but even a small amount leads to bias when using relative time and restricting to common years of overlap.
